{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9pWsPAt0dCh"
   },
   "source": [
    "# Assignment 2 - Find complex answers to medical questions\n",
    "\n",
    "**Submission deadline: Friday 22 April, 5pm.** \n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**.\n",
    "\n",
    "**Assessment marks: 20 marks (20% of the total unit assessment)**\n",
    "\n",
    "In this assignment we will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question.\n",
    "\n",
    "We will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0).\n",
    "\n",
    "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the CSV file are:\n",
    "\n",
    "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
    "* `sentid`: an ID for a sentence.\n",
    "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
    "* `sentence text`: The text of the sentence.\n",
    "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6xqxCmR0dCk"
   },
   "source": [
    "# Task 1 (5 marks): Data preparation\n",
    "\n",
    "Partition the data into the training, dev_test, and test sets using the proportions 6:2:2. That is, 60% of the questions must be in the training set, 20% must be in the dev_test set, and the remaining 20% in the test set. Make sure that you partition based on the questions, not on the rows. With this we mean that all the sentences related to a question must be in one file only. In other words, there must not be sentences from the same question in, say, the training and the test data.\n",
    "\n",
    "Also, make sure that you implement a random partition.\n",
    "\n",
    "Save the partitions as the files `training.csv`, `dev_test.csv`, and `test.csv`, so that they can be used by other people.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if your explanation answers the following question correctly: Why do we want to split the partition on the questions, and not on the rows?\n",
    "* **1 mark** if the code partitions the data on the questions randomly and according to the split 6:2:2.\n",
    "* **1 mark** if your code generates partitions that have similar balance of labels and you demonstrate that they are similar.\n",
    "* **1 mark** if the partitions are saved as the CSV files `training.csv`, `dev_test.csv`, and `test.csv`.\n",
    "* **1 mark** for good coding and documentation in this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fhqUtqJL0dCm"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Su2UC0Ti0dCo",
    "outputId": "4a705b65-e2e5-4415-a53b-a97b79bc2eaf"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4234\n"
     ]
    }
   ],
   "source": [
    "question = list(set(dataset['qid']))\n",
    "print(len(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3143, 20, 3400, 1327, 1798, 1589, 2084, 1534, 1978, 1906]\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(question)\n",
    "print(question[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixty = int(len(question)*.6)\n",
    "twenty = int(len(question)*.8)\n",
    "training_quesiton = question[:sixty]\n",
    "devtest_question = question[sixty:twenty]\n",
    "test_question = question[twenty:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "for d in training_quesiton:\n",
    "    document_data = dataset[dataset['qid'] == d]\n",
    "    training_set += list(document_data.itertuples(index=False, name=None))\n",
    "\n",
    "import csv\n",
    "\n",
    "fields = ['qid', 'sentid', 'question', 'sentence text', 'label'] \n",
    "with open('demo.csv','w',encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(fields)\n",
    "    w.writerows(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "devtest_set = []\n",
    "for d in devtest_question:\n",
    "    document_data = dataset[dataset['qid'] == d]\n",
    "    devtest_set += list(document_data.itertuples(index=False, name=None))\n",
    "\n",
    "\n",
    "fields = ['qid', 'sentid', 'question', 'sentence text', 'label'] \n",
    "with open('demo2.csv','w',encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(fields)\n",
    "    w.writerows(devtest_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "for d in test_question:\n",
    "    document_data = dataset[dataset['qid'] == d]\n",
    "    test_set += list(document_data.itertuples(index=False, name=None))\n",
    "\n",
    "\n",
    "fields = ['qid', 'sentid', 'question', 'sentence text', 'label'] \n",
    "with open('demo3.csv','w',encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(fields)\n",
    "    w.writerows(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbUJWlD_0dCv"
   },
   "source": [
    "# Task 2 (5 marks): Cosine similarity\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us in the file `data.zip` (so that any possible errors that you may have introduced in task 1 do not propagate to this task and following tasks).\n",
    "\n",
    "Implement a simple text summariser that is based on the cosine similarity between the question and the text. Use the following function signature.\n",
    "\n",
    "```{python}\n",
    "def cosine_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest cosine similarity\n",
    "    with the question. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[3, 1, 4], [12, 4, 13]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "To obtain the text vectors, use sklearn's tf.idf libraries this way:\n",
    "\n",
    "* Use all the defaults from the TfidfVectorizer instance, except for `stop_words=\"english\"` and `max_features=10000`. The latter option will restrict the vocabulary size to 10,000. This will speed up the computations and reduce the memory footprint in the subsequent tasks.\n",
    "* Use the `fit` method on the text of `training.csv`. In your documentation, please explain and justify what decision choices you made to select the correct text: would you use the question text only, the sentence text, or both?\n",
    "\n",
    "Evaluate the summariser by reporting the mean F1 score on each of the three CSV files `training.csv`, `devtest.csv`, and `test.csv`, for $n=5$. To calculate the mean F1 score, do this:\n",
    "\n",
    "1. For each question ID in the file, calculate the F1 score by comparing the result of your cosine summariser and the given labels. Feel free to use sklearn's functions to compute the F1 score, or implement your own version of the F1 scoring function if you prefer.\n",
    "2. Calculate the mean of the F1 scores calculated in step 1.\n",
    "\n",
    "Find the value of $n$ that returns the highest mean F1 score on the dev_test data.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the code generates the tf.idf vectors correctly. The explanations that justify the decisions made are reasonable. In particular, explain and justify what information you used to fit tf.idf.\n",
    "* **1 mark** if the code calculates cosine similarity correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest cosine similarity with the question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the dev_test file and identifies the value of $n$ that gives the highest score on the dev_test file.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, comment on the reason why you think the value of $n$ that gives highest F1 has that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UAr6x5Zc0dCw",
    "outputId": "7fa58ee8-d420-47ce-e925-c37950857924"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Task 3 (5 marks): Simple NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
    "\n",
    "1. An input layer that will accept the tf.idf of the sentence text (we will ignore the question text in this task). Use the TfidfVectorizer instance that you have fitted in task 2.\n",
    "2. A hidden layer and a relu activation function. You need to determine the size of the hidden layer.\n",
    "3. An output layer with one cell. The output layer will classify the input text (binary classification).\n",
    "\n",
    "Train the model with the training data and use the dev_test set to determine a good size of the hidden layer. \n",
    "\n",
    "With the model that you have trained, and implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
    "\n",
    "```{python}\n",
    "def nn_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest predicted score. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[2, 1, 3], [7, 14, 10]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "Report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the system developed in task 2. To make this task less time-consuming, focus only on $n=5$.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the tf.idf information of the text to the model correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the hidden layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uY6sDbUn0dC6"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0NeK3gM0dC9"
   },
   "source": [
    "# Task 4 (5 marks): Recurrent NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a more complex neural network that is composed of the following layers:\n",
    "\n",
    "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
    "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
    "* The final output layer with one cell for binary classification, as in task 3.\n",
    "\n",
    "Train the model with the training data, use the dev_test set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain what decisions had to be made to process long sentences. In particular, did you need to truncate the input text, and how did you determine the length limit?\n",
    "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the LSTM layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c8RRCWeQTrPl"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppkBsuB_0dC9"
   },
   "source": [
    "# Submission of results\n",
    "\n",
    "Your submission should consist of this Jupyter notebook with all your code and explanations inserted into the notebook as text cells. **The notebook should contain the output of the runs. All code should run. Code with syntax errors or code without output will not be assessed.**\n",
    "\n",
    "**Do not submit multiple files. If you feel you need to submit multiple files, please contact Diego.Molla-Aliod@mq.edu.au first.**\n",
    "\n",
    "Examine the text cells of this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the MarkDown notation](https://daringfireball.net/projects/markdown/syntax),  which explains the format of the text cells.\n",
    "\n",
    "Each task specifies a number of marks. The final mark of the assignment is the sum of all the marks of each individual task.\n",
    "\n",
    "By submitting this assignment you are acknowledging that this is your own work. Any submissions that break the code of academic honesty will be penalised as per [the academic integrity policy](https://policies.mq.edu.au/document/view.php?id=3).\n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
