{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9pWsPAt0dCh"
   },
   "source": [
    "# Assignment 2 - Find complex answers to medical questions\n",
    "\n",
    "**Submission deadline: Friday 22 April, 5pm.** \n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**.\n",
    "\n",
    "**Assessment marks: 20 marks (20% of the total unit assessment)**\n",
    "\n",
    "In this assignment we will work on a task of \"query-focused summarisation\" on medical questions where the goal is, given a medical question and a list of sentences extracted from relevant medical publications, to determine which of these sentences from the list can be used as part of the answer to the question.\n",
    "\n",
    "We will use data that has been derived from the **BioASQ challenge** (http://www.bioasq.org/), after some data manipulation to make it easier to process for this assignment. The BioASQ challenge organises several \"shared tasks\", including a task on biomedical semantic question answering which we are using here. The data are in the file `bioasq10_labelled.csv`, which is part of the zip file provided. Each row of the file has a question, a sentence text, and a label that indicates whether the sentence text is part of the answer to the question (1) or not (0).\n",
    "\n",
    "The following code uses pandas to store the file `bioasq10_labelled.csv` in a data frame and show the first rows of data. For this code to run, first you need to unzip the file `data.zip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"bioasq10b_labelled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the CSV file are:\n",
    "\n",
    "* `qid`: an ID for a question. Several rows may have the same question ID, as we can see above.\n",
    "* `sentid`: an ID for a sentence.\n",
    "* `question`: The text of the question. In the above example, the first rows all have the same question: \"Is Hirschsprung disease a mendelian or a multifactorial disorder?\"\n",
    "* `sentence text`: The text of the sentence.\n",
    "* `label`: 1 if the sentence is a part of the answer, 0 if the sentence is not part of the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6xqxCmR0dCk"
   },
   "source": [
    "# Task 1 (5 marks): Data preparation\n",
    "\n",
    "Partition the data into the training, dev_test, and test sets using the proportions 6:2:2. That is, 60% of the questions must be in the training set, 20% must be in the dev_test set, and the remaining 20% in the test set. Make sure that you partition based on the questions, not on the rows. With this we mean that all the sentences related to a question must be in one file only. In other words, there must not be sentences from the same question in, say, the training and the test data.\n",
    "\n",
    "Also, make sure that you implement a random partition.\n",
    "\n",
    "Save the partitions as the files `training.csv`, `dev_test.csv`, and `test.csv`, so that they can be used by other people.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if your explanation answers the following question correctly: Why do we want to split the partition on the questions, and not on the rows?\n",
    "* **1 mark** if the code partitions the data on the questions randomly and according to the split 6:2:2.\n",
    "* **1 mark** if your code generates partitions that have similar balance of labels and you demonstrate that they are similar.\n",
    "* **1 mark** if the partitions are saved as the CSV files `training.csv`, `dev_test.csv`, and `test.csv`.\n",
    "* **1 mark** for good coding and documentation in this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "fhqUtqJL0dCm"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "The reason why we split the partition on the questions not on rows because every rows has different sentense text with same questions, so, it is better that group by those questions and split it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# A list base on question\n",
    "question = list(set(dataset['qid']))\n",
    "print(question[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3143, 20, 3400, 1327, 1798]\n"
     ]
    }
   ],
   "source": [
    "# Shuffling the list\n",
    "random.shuffle(question)\n",
    "print(question[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportions of 6:2:2, 60% for training, 20% for dev_test and 20% for test\n",
    "sixty = int(len(question)*.6)\n",
    "twenty = int(len(question)*.8)\n",
    "training_quesiton = question[:sixty]\n",
    "devtest_question = question[sixty:twenty]\n",
    "test_question = question[twenty:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each row of the pandas data frame, save it in csv files\n",
    "training_set = []\n",
    "for d in training_quesiton:\n",
    "    document_data = dataset[dataset['qid'] == d]\n",
    "    training_set += list(document_data.itertuples(index=False, name=None))\n",
    "\n",
    "import csv\n",
    "\n",
    "fields = ['qid', 'sentid', 'question', 'sentence text', 'label'] \n",
    "with open('training.csv','w',encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(fields)\n",
    "    w.writerows(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "devtest_set = []\n",
    "for d in devtest_question:\n",
    "    document_data = dataset[dataset['qid'] == d]\n",
    "    devtest_set += list(document_data.itertuples(index=False, name=None))\n",
    "\n",
    "\n",
    "fields = ['qid', 'sentid', 'question', 'sentence text', 'label'] \n",
    "with open('dev_test.csv','w',encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(fields)\n",
    "    w.writerows(devtest_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "for d in test_question:\n",
    "    document_data = dataset[dataset['qid'] == d]\n",
    "    test_set += list(document_data.itertuples(index=False, name=None))\n",
    "\n",
    "\n",
    "fields = ['qid', 'sentid', 'question', 'sentence text', 'label'] \n",
    "with open('test.csv','w',encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(fields)\n",
    "    w.writerows(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbUJWlD_0dCv"
   },
   "source": [
    "# Task 2 (5 marks): Cosine similarity\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us in the file `data.zip` (so that any possible errors that you may have introduced in task 1 do not propagate to this task and following tasks).\n",
    "\n",
    "Implement a simple text summariser that is based on the cosine similarity between the question and the text. Use the following function signature.\n",
    "\n",
    "```{python}\n",
    "def cosine_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest cosine similarity\n",
    "    with the question. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[3, 1, 4], [12, 4, 13]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "To obtain the text vectors, use sklearn's tf.idf libraries this way:\n",
    "\n",
    "* Use all the defaults from the TfidfVectorizer instance, except for `stop_words=\"english\"` and `max_features=10000`. The latter option will restrict the vocabulary size to 10,000. This will speed up the computations and reduce the memory footprint in the subsequent tasks.\n",
    "* Use the `fit` method on the text of `training.csv`. In your documentation, please explain and justify what decision choices you made to select the correct text: would you use the question text only, the sentence text, or both?\n",
    "\n",
    "Evaluate the summariser by reporting the mean F1 score on each of the three CSV files `training.csv`, `devtest.csv`, and `test.csv`, for $n=5$. To calculate the mean F1 score, do this:\n",
    "\n",
    "1. For each question ID in the file, calculate the F1 score by comparing the result of your cosine summariser and the given labels. Feel free to use sklearn's functions to compute the F1 score, or implement your own version of the F1 scoring function if you prefer.\n",
    "2. Calculate the mean of the F1 scores calculated in step 1.\n",
    "\n",
    "Find the value of $n$ that returns the highest mean F1 score on the dev_test data.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the code generates the tf.idf vectors correctly. The explanations that justify the decisions made are reasonable. In particular, explain and justify what information you used to fit tf.idf.\n",
    "* **1 mark** if the code calculates cosine similarity correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest cosine similarity with the question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the dev_test file and identifies the value of $n$ that gives the highest score on the dev_test file.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, comment on the reason why you think the value of $n$ that gives highest F1 has that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "UAr6x5Zc0dCw",
    "outputId": "7fa58ee8-d420-47ce-e925-c37950857924"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "training = pd.read_csv(\"training.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "dev_test = pd.read_csv(\"dev_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  0,\n",
       "  'List signaling molecules (ligands) that interact with the receptor EGFR?',\n",
       "  'the epidermal growth factor receptor (EGFR) ligands, such as epidermal growth factor (EGF) and amphiregulin (AREG)',\n",
       "  1),\n",
       " (1,\n",
       "  1,\n",
       "  'List signaling molecules (ligands) that interact with the receptor EGFR?',\n",
       "  ' EGFR ligands epidermal growth factor (EGF), amphiregulin (AREG) and transforming growth factor alpha (TGFÎ±)',\n",
       "  1)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = []\n",
    "\n",
    "training_set += list(training.itertuples(index=False, name=None))\n",
    "training_set[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Task 3 (5 marks): Simple NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a simple TensorFlow-Keras neural model that has the following sequence of layers:\n",
    "\n",
    "1. An input layer that will accept the tf.idf of the sentence text (we will ignore the question text in this task). Use the TfidfVectorizer instance that you have fitted in task 2.\n",
    "2. A hidden layer and a relu activation function. You need to determine the size of the hidden layer.\n",
    "3. An output layer with one cell. The output layer will classify the input text (binary classification).\n",
    "\n",
    "Train the model with the training data and use the dev_test set to determine a good size of the hidden layer. \n",
    "\n",
    "With the model that you have trained, and implement a summariser that returns the $n$ sentences with highest predicted score. Use the following function signature:\n",
    "\n",
    "```{python}\n",
    "def nn_summariser(csvfile, questionids, n=5):\n",
    "   \"\"\"Return the IDs of the n sentences that have the highest predicted score. The input questionids is a list of question ids. The \n",
    "    output is a list of lists of sentence ids\n",
    "    >>> cosine_summariser('test.csv', [3, 11], 3)\n",
    "    [[2, 1, 3], [7, 14, 10]]\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "Report the final results using the test set. Remember: use the test set to report the final results of the best system only.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the system developed in task 2. To make this task less time-consuming, focus only on $n=5$.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the tf.idf information of the text to the model correctly.\n",
    "* **1 mark** if the code returns the IDs of the $n$ sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the hidden layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uY6sDbUn0dC6"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,), (0,), (1,)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(input='contents', max_features=10000)\n",
    "training_tfidf = tfidf.fit_transform([r[3] for r in training_set]).toarray()\n",
    "training_labels = [r[4:5] for r in training_set]\n",
    "training_labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "devtest_tfidf = tfidf.transform([r[3] for r in devtest_set]).toarray()\n",
    "devtest_labels = [r[4:5] for r in devtest_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 3)                 30003     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,003\n",
      "Trainable params: 30,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models, layers\n",
    "\n",
    "simple_nn = models.Sequential()\n",
    "simple_nn.add(layers.Dense(3, activation='relu', input_shape=(len(tfidf.get_feature_names()),)))\n",
    "simple_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "1186/1186 [==============================] - 5s 2ms/step - loss: 0.6547 - accuracy: 0.3678 - val_loss: 0.6013 - val_accuracy: 0.3251\n",
      "Epoch 2/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.5726 - accuracy: 0.3287 - val_loss: 0.6149 - val_accuracy: 0.3485\n",
      "Epoch 3/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.5706 - accuracy: 0.3527 - val_loss: 0.6608 - val_accuracy: 0.3352\n",
      "Epoch 4/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.5893 - accuracy: 0.3596 - val_loss: 0.7124 - val_accuracy: 0.3667\n",
      "Epoch 5/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.6195 - accuracy: 0.3657 - val_loss: 0.7941 - val_accuracy: 0.4899\n",
      "Epoch 6/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.6512 - accuracy: 0.3791 - val_loss: 0.8899 - val_accuracy: 0.4082\n",
      "Epoch 7/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.7005 - accuracy: 0.3614 - val_loss: 0.9870 - val_accuracy: 0.2949\n",
      "Epoch 8/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.7451 - accuracy: 0.4358 - val_loss: 1.0915 - val_accuracy: 0.4591\n",
      "Epoch 9/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.7955 - accuracy: 0.4449 - val_loss: 1.1896 - val_accuracy: 0.3876\n",
      "Epoch 10/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.8433 - accuracy: 0.4316 - val_loss: 1.2904 - val_accuracy: 0.3715\n",
      "Epoch 11/70\n",
      "1186/1186 [==============================] - 2s 2ms/step - loss: 0.8805 - accuracy: 0.4952 - val_loss: 1.3802 - val_accuracy: 0.4076\n",
      "Epoch 12/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.9141 - accuracy: 0.4665 - val_loss: 1.4648 - val_accuracy: 0.3746\n",
      "Epoch 13/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.9454 - accuracy: 0.4727 - val_loss: 1.5484 - val_accuracy: 0.3594\n",
      "Epoch 14/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.9722 - accuracy: 0.5040 - val_loss: 1.6240 - val_accuracy: 0.3857\n",
      "Epoch 15/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 0.9912 - accuracy: 0.4604 - val_loss: 1.6900 - val_accuracy: 0.3771\n",
      "Epoch 16/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.0153 - accuracy: 0.5057 - val_loss: 1.7637 - val_accuracy: 0.3627\n",
      "Epoch 17/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.0346 - accuracy: 0.5025 - val_loss: 1.8320 - val_accuracy: 0.4008\n",
      "Epoch 18/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.0581 - accuracy: 0.5229 - val_loss: 1.9027 - val_accuracy: 0.4126\n",
      "Epoch 19/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.0779 - accuracy: 0.5307 - val_loss: 1.9599 - val_accuracy: 0.4095\n",
      "Epoch 20/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.0943 - accuracy: 0.5189 - val_loss: 2.0007 - val_accuracy: 0.4458\n",
      "Epoch 21/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.1129 - accuracy: 0.5362 - val_loss: 2.0666 - val_accuracy: 0.4212\n",
      "Epoch 22/70\n",
      "1186/1186 [==============================] - 1s 1ms/step - loss: 1.1288 - accuracy: 0.5397 - val_loss: 2.0996 - val_accuracy: 0.4305\n",
      "Epoch 23/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.1455 - accuracy: 0.5517 - val_loss: 2.1568 - val_accuracy: 0.4467\n",
      "Epoch 24/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.1611 - accuracy: 0.5478 - val_loss: 2.2387 - val_accuracy: 0.3946\n",
      "Epoch 25/70\n",
      "1186/1186 [==============================] - 2s 2ms/step - loss: 1.1756 - accuracy: 0.5376 - val_loss: 2.2739 - val_accuracy: 0.4500\n",
      "Epoch 26/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.1857 - accuracy: 0.5492 - val_loss: 2.3118 - val_accuracy: 0.3927\n",
      "Epoch 27/70\n",
      "1186/1186 [==============================] - 2s 2ms/step - loss: 1.1999 - accuracy: 0.5353 - val_loss: 2.3588 - val_accuracy: 0.3899\n",
      "Epoch 28/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.2142 - accuracy: 0.5513 - val_loss: 2.4009 - val_accuracy: 0.4775\n",
      "Epoch 29/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.2257 - accuracy: 0.5612 - val_loss: 2.4340 - val_accuracy: 0.4546\n",
      "Epoch 30/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.2344 - accuracy: 0.5660 - val_loss: 2.4846 - val_accuracy: 0.3774\n",
      "Epoch 31/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.2439 - accuracy: 0.5578 - val_loss: 2.5065 - val_accuracy: 0.4501\n",
      "Epoch 32/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.2558 - accuracy: 0.5542 - val_loss: 2.5609 - val_accuracy: 0.3962\n",
      "Epoch 33/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.2686 - accuracy: 0.5696 - val_loss: 2.5936 - val_accuracy: 0.4449\n",
      "Epoch 34/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.2786 - accuracy: 0.5770 - val_loss: 2.6236 - val_accuracy: 0.4458\n",
      "Epoch 35/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.2867 - accuracy: 0.5780 - val_loss: 2.6360 - val_accuracy: 0.4752\n",
      "Epoch 36/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.2960 - accuracy: 0.5873 - val_loss: 2.6698 - val_accuracy: 0.4329\n",
      "Epoch 37/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3004 - accuracy: 0.5754 - val_loss: 2.7002 - val_accuracy: 0.4383\n",
      "Epoch 38/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3083 - accuracy: 0.5804 - val_loss: 2.7358 - val_accuracy: 0.3840\n",
      "Epoch 39/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3158 - accuracy: 0.5803 - val_loss: 2.7597 - val_accuracy: 0.4715\n",
      "Epoch 40/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3216 - accuracy: 0.5762 - val_loss: 2.7852 - val_accuracy: 0.4839\n",
      "Epoch 41/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3261 - accuracy: 0.5779 - val_loss: 2.8190 - val_accuracy: 0.4989\n",
      "Epoch 42/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3336 - accuracy: 0.5849 - val_loss: 2.8329 - val_accuracy: 0.4807\n",
      "Epoch 43/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3404 - accuracy: 0.5917 - val_loss: 2.8725 - val_accuracy: 0.4385\n",
      "Epoch 44/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3437 - accuracy: 0.5834 - val_loss: 2.8783 - val_accuracy: 0.4489\n",
      "Epoch 45/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3495 - accuracy: 0.5939 - val_loss: 2.9006 - val_accuracy: 0.4854\n",
      "Epoch 46/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3563 - accuracy: 0.6007 - val_loss: 2.9312 - val_accuracy: 0.4751\n",
      "Epoch 47/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3595 - accuracy: 0.5940 - val_loss: 2.9576 - val_accuracy: 0.4754\n",
      "Epoch 48/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3639 - accuracy: 0.5908 - val_loss: 2.9704 - val_accuracy: 0.4528\n",
      "Epoch 49/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3675 - accuracy: 0.5915 - val_loss: 2.9845 - val_accuracy: 0.4079\n",
      "Epoch 50/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3721 - accuracy: 0.6000 - val_loss: 3.0180 - val_accuracy: 0.4977\n",
      "Epoch 51/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3751 - accuracy: 0.6046 - val_loss: 3.0229 - val_accuracy: 0.4639\n",
      "Epoch 52/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3791 - accuracy: 0.5996 - val_loss: 3.0315 - val_accuracy: 0.4488\n",
      "Epoch 53/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3834 - accuracy: 0.6047 - val_loss: 3.0425 - val_accuracy: 0.5128\n",
      "Epoch 54/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3863 - accuracy: 0.6098 - val_loss: 3.0744 - val_accuracy: 0.4442\n",
      "Epoch 55/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3914 - accuracy: 0.6142 - val_loss: 3.0807 - val_accuracy: 0.4613\n",
      "Epoch 56/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3928 - accuracy: 0.6133 - val_loss: 3.0948 - val_accuracy: 0.4755\n",
      "Epoch 57/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.3970 - accuracy: 0.6112 - val_loss: 3.1266 - val_accuracy: 0.4571\n",
      "Epoch 58/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4008 - accuracy: 0.6077 - val_loss: 3.1281 - val_accuracy: 0.4816\n",
      "Epoch 59/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4025 - accuracy: 0.6106 - val_loss: 3.1493 - val_accuracy: 0.4422\n",
      "Epoch 60/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4077 - accuracy: 0.6074 - val_loss: 3.1677 - val_accuracy: 0.4518\n",
      "Epoch 61/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4105 - accuracy: 0.6113 - val_loss: 3.1775 - val_accuracy: 0.4474\n",
      "Epoch 62/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4129 - accuracy: 0.6029 - val_loss: 3.1979 - val_accuracy: 0.4375\n",
      "Epoch 63/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4170 - accuracy: 0.6129 - val_loss: 3.1969 - val_accuracy: 0.4456\n",
      "Epoch 64/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4186 - accuracy: 0.6060 - val_loss: 3.2267 - val_accuracy: 0.5012\n",
      "Epoch 65/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4220 - accuracy: 0.6093 - val_loss: 3.2536 - val_accuracy: 0.4628\n",
      "Epoch 66/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4253 - accuracy: 0.6031 - val_loss: 3.2409 - val_accuracy: 0.4650\n",
      "Epoch 67/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4293 - accuracy: 0.6087 - val_loss: 3.2759 - val_accuracy: 0.4679\n",
      "Epoch 68/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4329 - accuracy: 0.6033 - val_loss: 3.2784 - val_accuracy: 0.4356\n",
      "Epoch 69/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4345 - accuracy: 0.6025 - val_loss: 3.2934 - val_accuracy: 0.4500\n",
      "Epoch 70/70\n",
      "1186/1186 [==============================] - 2s 1ms/step - loss: 1.4379 - accuracy: 0.6107 - val_loss: 3.3110 - val_accuracy: 0.4638\n"
     ]
    }
   ],
   "source": [
    "history=simple_nn.fit(training_tfidf,\n",
    "                      np.array(training_labels),\n",
    "                      epochs=70,\n",
    "                      batch_size=32,\n",
    "                      validation_data=(devtest_tfidf, np.array(devtest_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0NeK3gM0dC9"
   },
   "source": [
    "# Task 4 (5 marks): Recurrent NN\n",
    "\n",
    "Use the files `training.csv`, `dev_test.csv`, and `test.csv` provided by us.\n",
    "\n",
    "Implement a more complex neural network that is composed of the following layers:\n",
    "\n",
    "* An embedding layer that generates embedding vectors of the sentence text with 35 dimensions.\n",
    "* A LSTM layer. You need to determine the size of this LSTM layer, and the text length limit (if needed).\n",
    "* The final output layer with one cell for binary classification, as in task 3.\n",
    "\n",
    "Train the model with the training data, use the dev_test set to determine a good size of the LSTM layer and an appropriate length limit (if needed), and report the final results using the test set. Again, remember to use the test set only after you have determined the optimal parameters of the LSTM layer.\n",
    "\n",
    "Based on your experiments, comment on whether this system is better than the systems developed in the previous tasks.\n",
    "\n",
    "The breakdown of marks is as follows:\n",
    "\n",
    "* **1 mark** if the NN model has the correct layers, the correct activation functions, and the correct loss function.\n",
    "* **1 mark** if the code passes the sentence text to the model correctly. The documentation needs to explain what decisions had to be made to process long sentences. In particular, did you need to truncate the input text, and how did you determine the length limit?\n",
    "* **1 mark** if the code returns the IDs of the *n* sentences that have the highest prediction score in the given question.\n",
    "* **1 mark** if the notebook reports the F1 scores of the test sets and comments on the results.\n",
    "* **1 mark** for good coding and documentation in this task. In particular, the code and results must include evidence that shows your choice of best size of the LSTM layer. The explanations must be clear and concise. To make this task less time-consuming, use $n=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c8RRCWeQTrPl"
   },
   "outputs": [],
   "source": [
    "# Write your code and answers here. Feel free to add more code and markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[66, 66, 66]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_test_lengths = [len(r[2]) for r in devtest_set]\n",
    "dev_test_lengths[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dev_test_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppkBsuB_0dC9"
   },
   "source": [
    "# Submission of results\n",
    "\n",
    "Your submission should consist of this Jupyter notebook with all your code and explanations inserted into the notebook as text cells. **The notebook should contain the output of the runs. All code should run. Code with syntax errors or code without output will not be assessed.**\n",
    "\n",
    "**Do not submit multiple files. If you feel you need to submit multiple files, please contact Diego.Molla-Aliod@mq.edu.au first.**\n",
    "\n",
    "Examine the text cells of this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the MarkDown notation](https://daringfireball.net/projects/markdown/syntax),  which explains the format of the text cells.\n",
    "\n",
    "Each task specifies a number of marks. The final mark of the assignment is the sum of all the marks of each individual task.\n",
    "\n",
    "By submitting this assignment you are acknowledging that this is your own work. Any submissions that break the code of academic honesty will be penalised as per [the academic integrity policy](https://policies.mq.edu.au/document/view.php?id=3).\n",
    "\n",
    "Late submissions **will not be accepted** without an approved [Special Consideration](http://from.mq.edu.au/MT0X0E0FUrrU200rm0JB0U0) request.  Assessments submitted after the due date will receive a mark of **zero**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a7b63e7410c98f344f02082f10d790581d1dba1eeb1c8fe30f342f6109f0429e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
